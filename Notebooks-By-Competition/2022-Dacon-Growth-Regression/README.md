# Dacon : Growth Prediction

## ğŸ† Goal
- Predict Growth of Bok Choi

## ğŸ“‹ LeaderBoard
- 27th out of 171 teams

## Data
- Time Series
    - Environment : Temperature, Humidity, Light...
- Target
    - Growth Rate 

## ğŸ”– Retrospection

### Difficulty

Google Colab changed their policy on charging! It became much more expensive than before!

Because of fee, I had only two chances to train new model and hyperparameter tuning is not allowed (it takes such a long time.)

My worry became reality. I hoped this would not happen... ğŸ˜¥...

### Data Prep.

The dataset is basically time-series, encoded with integer. some variables, such as `Fan State` looks like categorical or ordinal. Based on my research, the data is collected by `Arduino` or `Rasberry Pi`, both are popular system for monitoring plants. Anyway, since each of `equipment status` (including heating system) range from 0 to 255. Regardless of meaning, I considered it as ordinal.

Preprocessing precedure was not special, but there are my new experiments.

- Approach
    1. Let's think this is Panel data
    2. Target Variable Augmentation
    3. K-fold by case


#### Anomaly Detection


While exploring raw data, I found some anomalies in inputs and even in target. For example, the `White LED Light` which refers to white LED strength is 50 times larger at one point. This happens all over the dataset.

The reason why I did not take standard deviation or 25-75 quantile is that there were so many zeores. In case of `Light` this should be turned off almost 12 hours in general days, perfectly turned off until the seed sprouts. (negative photoblastic) 

For this reason I adapted ***Prophet*** for two perspectives: (1) the property of time series dataset (2) robust, therefore no tuning.
- Fill Anomalies
    - Input
        All the anomalies are replaced with mean of $t_{i}$ (not exact timing, but the $i_{th}$ of sequence) all over the dataset
    - Target
        In case of target, linear interpolation is used.

#### Aggregation

Target variable is recorded by day and input is by minute. Aggregate input variable into daily level would lead to better performance - well, this is not that fun.

My idea is aggregating input into hours, and also augment target at the same time.

- Augment Target (Korean)
    - Main Idea
        - ì…ë ¥ ë°ì´í„°ëŠ” ë¶„ ë‹¨ìœ„ì´ë‚˜ íƒ€ê²Ÿ ë³€ìˆ˜ì¸ ì„±ì¥ë¥ ì€ ì¼ ë‹¨ìœ„ë¡œ ê¸°ë¡ë˜ì–´ ìˆìœ¼ë¯€ë¡œ augmentationì„ í†µí•´ì„œ ë¶„ë‹¨ìœ„ì— ì í•©í•˜ê²Œ ì¦ê°•(augmentation)
        - ì´ë¡ ì ìœ¼ë¡œ ì‹ë¬¼ì€ ë¹›ì´ ê±°ì˜ ì—†ëŠ” ì•¼ê°„ì— ì„±ì¥í•¨ â¡ ì´ê´‘ëŸ‰ì¸ Total Lightì˜ ê°’ì— ë”°ë¼ì„œ íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì¦ê°•
        - ì‹ë¬¼ì˜ ìƒë¦¬ìƒ ë¹›ì´ ì—†ëŠ” êµ¬ê°„ì—ì„œ í•­ìƒ ë™ì¼í•˜ê²Œ ìƒìˆ˜ë§Œí¼ ì„±ì¥í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ì„ í˜•ì ì¸ ì¦ê°€-ê°ì†Œê°€ ë°œìƒí•¨.
        - ì´ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” êµ¬ê°„ë³„ë¡œ ë‹¤ë¥¸ ê°’ì„ ì§€ë…€ì•¼í•˜ë©°, ì¸ë±ìŠ¤ê°€ ë‹¨ì¡° ì¦ê°€ê°€ ì•„ë‹Œ ì¦ê°ì„ ëª¨ë‘ ë°˜ì˜ í•  ìˆ˜ ìˆëŠ” í˜•íƒœì—¬ì•¼í•¨.
        - ë‹¤ìŒì˜ ì—°ì‚°ìœ¼ë¡œ ìœ„ì˜ ì‚¬í•­ë“¤ì„ ë°˜ì˜í•˜ê³ ìí•¨.
    - Formula
        1. ì¼ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¦¬
        2. ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œ
        3. ì¶”ì¶œëœ ì •ìˆ˜ ì¸ë±ìŠ¤ì— ëŒ€í•˜ì—¬ ë‹¤ìŒì˜ ì—°ì‚°ì„ í†µí•˜ì—¬ ë³€í™˜   
    ---
    - tëŠ” ì¼(day), iëŠ” ì‹œ(hour)ì„ ì˜ë¯¸
    - $A_{t}$ëŠ” ì¸ë±ìŠ¤ ië¡œ êµ¬ì„±ëœ t ì‹œì ì˜ Arrayë¥¼ ì˜ë¯¸
    - $L_{i}$ëŠ” ì¸ë±ìŠ¤ iì˜ ì‹œì ì— ëŒ€í•´ ê´‘ëŸ‰ì„ ë³€í™˜í•œ ê°’
        - ê´‘ëŸ‰(Total Light)ë¥¼ Xë¼ê³  í•  ë•Œ,
        - Xê°€ 0ì¼ ê²½ìš° 1
        - Xì´ 0ë³´ë‹¤ í´ ê²½ìš° $\frac{1}{X_{i}}$

    - $R_{t}$ì€ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ í•˜ë£¨ì¹˜ ê°’ì„ ì˜ë¯¸.   
    $$\frac{|(A_{i}*L_{i})\ -\ \lfloor(length(A) / 2)âŒ‹|}{Î£A_{i}} \ \ \  \ \ \ \ \times \ R_{t}$$
    --- 

    - ì—°ì‚°ì˜ ê²°ê³¼ëŠ” ë‹¤ìŒìœ¼ë¡œ í•´ì„ í•  ìˆ˜ ìˆìŒ
        - ì‹ë¬¼ì˜ ì„±ì¥ì´ ë¹›ì´ ì—†ì„ ë•Œì—ë§Œ ì¼ì–´ë‚œë‹¤ê³  ê°€ì • í•  ë•Œ, 
        - ì‹œì  tì˜ ì´ì„±ì¥ë¥  $R_{t}$ë¥¼ ë¹›ì´ ì—†ëŠ” êµ¬ê°„ $L_{i}$ì— ëŒ€í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒˆë¡­ê²Œ ìƒì„±í•¨ (ì¦ê°•í•¨)

#### Lag

added lagged variable before and after 24 hours at the point of $t_{i}$.

### K-Fold

- Since I treated the dataset as panel data, K-fold strategy should be different.
    - *Note*. If you are not familar with time series, don't forget the conventional K-fold on time series does not work.
        - Check Article : [Donâ€™t Use K-fold Validation for Time Series Forecasting](https://towardsdatascience.com/dont-use-k-fold-validation-for-time-series-forecasting-30b724aaea64)

train-valid split does not come from the same case, consider all the case as one data and choose train case and test case.
![Kfold](img/img.png)

To be honest, this is a common approach in A/B test concept(frequentist statistics). I leave this since I couldn't find this idea when it comes to ML articles.

### Model

***LightGBM*** is used, the paramters were not tightly set only for training speed. 

- prediction aggregation 
    the prediction is generated by hourly level, therefore it should be aggregated to daily level.

### Comment

- 67th in public score (2 test cases) vs 27th in private (all test cases)

I wondered the augmentation method works well, or it just throws a bunch of trash. I thought it was latter when I got public score. The gap between public - private is so huge! This difference tells me that my approach works! ğŸ˜Š 

Probably the aggregation level is smaller (10 minutes, 15 minutes or 30 minutes) and smaller learning rate $\eta$ gave me a better score. Additionally, ensemble might be helpful. 